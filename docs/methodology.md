# Methodology

These intelligence briefs are produced using a structured research methodology:

1. **Web research**: Multiple parallel research agents survey academic papers, industry reports, and practitioner evidence on each topic
2. **Evidence tiering**: Sources classified as Tier 1 (peer-reviewed, pre-registered), Tier 2 (major industry research, n>500), or Tier 3 (practitioner reports, benchmarks)
3. **Synthesis**: Findings structured into analytical frameworks with explicit confidence levels and falsification criteria
4. **Quality review**: Each brief reviewed against evidence standards before publication

## Evidence Tiers

| Tier | Description | Examples |
|------|-------------|---------|
| 1 | Peer-reviewed papers, pre-registered trials, standards body reports | METR RCT, ICLR publications, NIST/DORA reports |
| 2 | Major industry research with significant sample sizes | McKinsey surveys (n>1,000), Goldman Sachs analysis, Sonar Developer Survey |
| 3 | Benchmark leaderboards, practitioner reports, expert analysis | SWE-bench, Aider leaderboard, blog-based analysis |

Each brief reports its evidence composition (e.g., "25% Tier 1, 45% Tier 2, 30% Tier 3") so readers can assess the strength of the evidence base.

## AI-Assisted Workflow

This workflow is powered by an advisory agent built on an open-source template for creating advisory AI agents. The agent handles parallel web research, source classification, and draft structuring. The author reviews, edits, and takes full responsibility for all published analysis.

## What Makes This Different

Most AI commentary is opinion-first. These briefs are evidence-first:

- Every claim cites a specific source with year and sample size where available
- Confidence levels are stated explicitly (High/Medium/Low)
- Falsification criteria are included: "Would change if..."
- Counter-evidence is presented alongside supporting evidence
