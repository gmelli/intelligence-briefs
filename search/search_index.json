{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Intelligence Briefs","text":"<p>Evidence-tiered research briefs on AI engineering, productivity, and agent architecture.</p>"},{"location":"#what-this-is","title":"What This Is","text":"<p>Each brief synthesizes academic papers, industry reports, and practitioner evidence into structured analysis with explicit confidence levels. Sources are classified by evidence tier:</p> <ul> <li>Tier 1: Peer-reviewed papers, pre-registered trials, standards body reports</li> <li>Tier 2: Major industry research (n&gt;500), institutional reports</li> <li>Tier 3: Benchmark leaderboards, practitioner reports, expert analysis</li> </ul>"},{"location":"#briefs","title":"Briefs","text":""},{"location":"#ai-coding","title":"AI Coding","text":"<ul> <li>Coding Accuracy vs. Inference Speed: The Three Pareto Frontiers \u2014 No single AI coding model wins all benchmarks. The real optimization problem is workflow design, not model selection. (16 sources, Tier 1)</li> </ul>"},{"location":"#ai-productivity","title":"AI Productivity","text":"<p>Coming soon</p>"},{"location":"#agent-architecture","title":"Agent Architecture","text":"<p>Coming soon</p>"},{"location":"#observations","title":"Observations","text":"<p>Lighter-weight pattern signals and research notes.</p> <p>Coming soon</p> <p>Methodology | About</p>"},{"location":"about/","title":"About","text":"<p>Gabor Melli is an AI engineering leader with experience in machine learning systems, agent architectures, and AI-assisted development practices.</p> <p>These intelligence briefs represent ongoing research into AI engineering, productivity measurement, and agentic systems \u2014 areas where the gap between hype and evidence is widest and the need for rigorous analysis is greatest.</p>"},{"location":"about/#contact","title":"Contact","text":"<ul> <li>LinkedIn</li> <li>GitHub</li> </ul>"},{"location":"methodology/","title":"Methodology","text":"<p>These intelligence briefs are produced using a structured research methodology:</p> <ol> <li>Web research: Multiple parallel research agents survey academic papers, industry reports, and practitioner evidence on each topic</li> <li>Evidence tiering: Sources classified as Tier 1 (peer-reviewed, pre-registered), Tier 2 (major industry research, n&gt;500), or Tier 3 (practitioner reports, benchmarks)</li> <li>Synthesis: Findings structured into analytical frameworks with explicit confidence levels and falsification criteria</li> <li>Quality review: Each brief reviewed against evidence standards before publication</li> </ol>"},{"location":"methodology/#evidence-tiers","title":"Evidence Tiers","text":"Tier Description Examples 1 Peer-reviewed papers, pre-registered trials, standards body reports METR RCT, ICLR publications, NIST/DORA reports 2 Major industry research with significant sample sizes McKinsey surveys (n&gt;1,000), Goldman Sachs analysis, Sonar Developer Survey 3 Benchmark leaderboards, practitioner reports, expert analysis SWE-bench, Aider leaderboard, blog-based analysis <p>Each brief reports its evidence composition (e.g., \"25% Tier 1, 45% Tier 2, 30% Tier 3\") so readers can assess the strength of the evidence base.</p>"},{"location":"methodology/#ai-assisted-workflow","title":"AI-Assisted Workflow","text":"<p>This workflow is powered by an advisory agent built on an open-source template for creating advisory AI agents. The agent handles parallel web research, source classification, and draft structuring. The author reviews, edits, and takes full responsibility for all published analysis.</p>"},{"location":"methodology/#what-makes-this-different","title":"What Makes This Different","text":"<p>Most AI commentary is opinion-first. These briefs are evidence-first:</p> <ul> <li>Every claim cites a specific source with year and sample size where available</li> <li>Confidence levels are stated explicitly (High/Medium/Low)</li> <li>Falsification criteria are included: \"Would change if...\"</li> <li>Counter-evidence is presented alongside supporting evidence</li> </ul>"},{"location":"briefs/ai-coding/coding-pareto-frontiers-summary/","title":"Executive Summary: The Three Pareto Frontiers in AI Coding","text":"<p>Based on: 16 sources including METR RCT, ICLR 2025, DORA 2025, Stanford HAI</p>"},{"location":"briefs/ai-coding/coding-pareto-frontiers-summary/#the-problem","title":"The Problem","text":"<p>AI coding tools promise speed. The evidence shows they deliver speed \u2014 but at the cost of quality, review burden, and downstream defects. Six independent studies converge on the same finding.</p>"},{"location":"briefs/ai-coding/coding-pareto-frontiers-summary/#three-frontiers-one-insight","title":"Three Frontiers, One Insight","text":"<ol> <li> <p>Model Selection (well-mapped): No single model wins all benchmarks. Claude Opus leads SWE-bench (80.9%), Gemini leads LiveCodeBench (91.7%), DeepSeek offers 73% accuracy at 1/170th the cost. Cascade routing (ICLR 2025) can save 50x in costs.</p> </li> <li> <p>Workflow Design (poorly mapped \u2014 largest gap): The \"AI speed trap\" is documented by 6 studies: perceived +20% speed gains mask -19% actual throughput (METR RCT), +41% bug rates (Uplevel), and +91% review time (DORA). The quality-first AI configuration has never been studied.</p> </li> <li> <p>Organizational Productivity (emerging): Connects to the broader AI productivity paradox \u2014 more AI adoption correlates with less delivery stability (-7.2% per 25% adoption, DORA 2025).</p> </li> </ol>"},{"location":"briefs/ai-coding/coding-pareto-frontiers-summary/#key-takeaway","title":"Key Takeaway","text":"<p>Invest in workflow engineering before model procurement. Agentic wrappers (+16.5 percentage points on Aider) deliver larger gains than switching to more expensive models. The unmapped quality-first quadrant is the highest-value research target.</p>"},{"location":"briefs/ai-coding/coding-pareto-frontiers-summary/#confidence","title":"Confidence","text":"<p>High for evidence synthesis, Medium for predictive claims. Would change if quality-optimized AI workflow studies are published.</p> <p>Read the full brief for complete data tables, source analysis, and methodology.</p>"},{"location":"briefs/ai-coding/coding-pareto-frontiers/","title":"Coding Accuracy vs. Inference Speed: The Three Pareto Frontiers","text":"<p>Date: 2026-02-23 Sources: Artificial Analysis (Feb 2026), METR RCT (Jul 2025) + Transcript Analysis (Feb 2026), CodeRabbit State of Code Quality (Dec 2025), Sonar 2026 Developer Survey (Jan 2026), Harness AI Velocity Paradox (Sep 2025), DORA 2025, Uplevel Copilot Study (2024), GitClear 2025, Greptile State of AI Coding 2025, Aider Polyglot Leaderboard, SWE-bench Verified (Feb 2026), LiveCodeBench (Feb 2026), Terminal-Bench 2.0 (Feb 2026), Dekoninck et al. ICLR 2025 (Cascade Routing), Stanford HAI AI Index 2025, BAMBO arXiv:2512.09972 Evidence Tier: 1 (METR pre-registered RCT, ICLR 2025 peer-reviewed, NIST/Stanford/DORA institutional reports, BLS data) + Tier 2 (industry surveys n&gt;800) + Tier 3 (benchmark leaderboards, practitioner reports)</p> <p>This brief examines three nested optimization problems in AI-assisted coding: model selection (well-mapped by benchmarks), workflow design (poorly mapped, the largest research gap), and organizational productivity (emerging). The evidence reveals that industry investment overwhelmingly targets model selection while workflow design offers larger returns.</p>"},{"location":"briefs/ai-coding/coding-pareto-frontiers/#1-the-model-level-pareto-frontier-well-mapped","title":"1. The Model-Level Pareto Frontier (Well-Mapped)","text":""},{"location":"briefs/ai-coding/coding-pareto-frontiers/#coding-benchmark-leaderboard-february-2026","title":"Coding Benchmark Leaderboard (February 2026)","text":"Model SWE-bench V LiveCodeBench Terminal-Bench 2.0 Aider Polyglot Output t/s $/M Input Claude Opus 4.5 80.9% ~87% 58.4% 89.4% 67 $5.00 Claude Opus 4.6 80.8% \u2014 65.4% \u2014 75 $5.00 GPT-5.2 80.0% 89% \u2014 88.0% (thinking) ~187 ~$5.00 GPT-5.3 Codex \u2014 \u2014 77.3% \u2014 ~62 \u2014 Gemini 3.1 Pro \u2014 91.7% 67.4% \u2014 110 $2.00 Gemini 3 Pro 76.2% \u2014 55.1% 82.2% 257 $2.00 Claude Sonnet 4.5 77.2% \u2014 \u2014 82.4% 67 $3.00 Claude Sonnet 4.6 \u2014 \u2014 59.6% \u2014 62 ~$1.20 DeepSeek V3.2 73.1% 89.6% \u2014 \u2014 ~40 $0.03 GLM-4.7/5 77.8% \u2014 \u2014 \u2014 63 $0.05 <p>Key finding: No single model wins across all benchmarks. Claude Opus leads SWE-bench (80.9%) but underperforms on Terminal-Bench (58.4%). Gemini leads LiveCodeBench (91.7%) and Terminal-Bench (67.4%) but trails on SWE-bench (76.2%). The frontier is multi-dimensional (accuracy x speed x cost x task-type).</p>"},{"location":"briefs/ai-coding/coding-pareto-frontiers/#price-performance-champions","title":"Price-Performance Champions","text":"<ul> <li>Budget frontier: DeepSeek V3.2 at $0.03/M input \u2014 73% SWE-bench at ~1/170th the cost of Opus</li> <li>Mid-tier frontier: Gemini 3.1 Pro at $2.00/M \u2014 leads most benchmarks, 7.5x cheaper than Opus</li> <li>Accuracy ceiling: Claude Opus 4.5/4.6 \u2014 80.9% SWE-bench, premium pricing</li> </ul>"},{"location":"briefs/ai-coding/coding-pareto-frontiers/#live-frontier-tracking","title":"Live Frontier Tracking","text":"<p>Multiple projects explicitly map the LLM pareto frontier: Winston Bosan's live visualization (winston-bosan.github.io/llm-pareto-frontier/), Artificial Analysis Intelligence Index v4.0 (25% coding weight), and BAMBO (arXiv:2512.09972) which uses Bayesian optimization to construct provably optimal pareto sets.</p>"},{"location":"briefs/ai-coding/coding-pareto-frontiers/#2-the-workflow-level-pareto-frontier-poorly-mapped-largest-research-gap","title":"2. The Workflow-Level Pareto Frontier (Poorly Mapped \u2014 Largest Research Gap)","text":""},{"location":"briefs/ai-coding/coding-pareto-frontiers/#the-ai-speed-trap-pattern-documented-by-6-independent-studies","title":"The AI Speed Trap Pattern (Documented by 6+ Independent Studies)","text":"Study Speed Metric Quality Metric Net Assessment METR RCT (2025, n=16, pre-registered) Perceived +20% Actual -19% wall-clock Negative Uplevel (2024, n=800, before/after) No cycle time change +41% bug rate Negative CodeRabbit (2025, n=470 PRs) +20% PRs/author/yr 1.7x issues per PR Ambiguous Harness (2025, n=900 survey) 63% ship faster 72% had AI production incidents Negative net DORA 2025 90% using AI tools -7.2% delivery stability per 25% adoption Destabilizing GitClear (2025, 211M LoC) More code produced Code churn nearly doubled since 2020 Negative <p>The pattern: (1) Optimize for speed \u2192 (2) Perceive +20% gain \u2192 (3) Actual throughput neutral-to-negative \u2192 (4) Quality degrades 41-70% \u2192 (5) Downstream costs escalate \u2192 (6) Net Pareto-inferior to careful human coding.</p>"},{"location":"briefs/ai-coding/coding-pareto-frontiers/#the-verification-tax","title":"The Verification Tax","text":"Metric Value Source Review time increase for AI code +91% DORA 2025 PR size increase +154% average DORA 2025 Devs who say AI review &gt; human review effort 38% Sonar 2026 (n=1,100) Devs who DON'T trust AI code 96% Sonar 2026 Devs who ALWAYS verify anyway Only 48% Sonar 2026 AI code: 42% of all committed code \u2014 Sonar 2026 Production incidents from AI code 72% of orgs Harness 2025 Hidden defects surface after 30-90 days CodeRabbit / Codebridge <p>AWS CTO Werner Vogels: \"When you write code yourself, comprehension comes with creation. When the machine writes it, you must rebuild that comprehension during review.\" This makes the verification cost partly structural, not solely model-quality-dependent.</p>"},{"location":"briefs/ai-coding/coding-pareto-frontiers/#the-unmapped-quadrant","title":"The Unmapped Quadrant","text":"Low Quality High Quality High Speed Well-studied (METR, Uplevel, Harness) \u2014 net negative NOT YET STUDIED Low Speed Not relevant Baseline human performance <p>The entire evidence base describes speed-optimized or unstructured AI use. No study has measured the quality-optimized configuration (slower models, mandatory verification, multi-agent review). The quality-first AI-assisted quadrant is the single largest research gap in the field.</p>"},{"location":"briefs/ai-coding/coding-pareto-frontiers/#metr-follow-up-february-17-2026","title":"METR Follow-Up (February 17, 2026)","text":"<p>METR transcript analysis of 5,305 Claude Code transcripts found 1.5-13x time savings on self-selected tasks \u2014 but with critical caveats: selection bias (developers choose tasks where AI helps), the time savings factor \"does not equal the productivity multiplier,\" and the original RCT's serial task design may understate AI's concurrency benefits. No replication of the original RCT has been published.</p>"},{"location":"briefs/ai-coding/coding-pareto-frontiers/#zvi-mowshowitz-critique","title":"Zvi Mowshowitz Critique","text":"<p>Key limitations of the METR RCT: deeply-understood open-source repos are worst-case for AI tools; 1-2 hour pre-decomposed tasks limit flexibility; hourly pay reduces efficiency incentives; developers lacked tool experience. Conclusion: the -19% may be specific to study conditions rather than a general indictment.</p>"},{"location":"briefs/ai-coding/coding-pareto-frontiers/#3-strategies-to-move-the-frontier-not-just-trade-along-it","title":"3. Strategies to Move the Frontier (Not Just Trade Along It)","text":""},{"location":"briefs/ai-coding/coding-pareto-frontiers/#cascade-routing-iclr-2025-dekoninck-et-al","title":"Cascade Routing (ICLR 2025 \u2014 Dekoninck et al.)","text":"<p>Unifies routing (single model per query) and cascading (sequential escalation) into a theoretically optimal strategy: - 50x cost savings possible (Haiku vs. Opus) - 16x efficiency for code generation vs. naive cascading - 5x+ cost savings with reliable judges at negligible quality loss - Not Diamond router: +25% accuracy, -10x cost via pareto-optimized routing</p> <p>Practical three-tier pattern: Haiku/Flash for 80% of tasks, Sonnet for 15%, Opus for 5%.</p>"},{"location":"briefs/ai-coding/coding-pareto-frontiers/#verification-first-workflows","title":"Verification-First Workflows","text":"<p>Only demonstrated technique for converting AI speed into net quality improvement: - Spotify Honk: LLM Judge vetoes ~25% of sessions - Multi-agent review: \"one writes, another critiques, another tests, another validates\" - SonarQube users: 24% lower vulnerability rates, 20% lower defect rates</p>"},{"location":"briefs/ai-coding/coding-pareto-frontiers/#adaptive-reasoning","title":"Adaptive Reasoning","text":"<p>Right amount of thinking per task: - Claude Sonnet 4.6 adaptive: ~2s simple, extended thinking for complex - GPT-5.1 adaptive: ~2s simple, 10s+ complex, 50% fewer tokens at similar quality - Agentic wrappers: Refact.ai boosted Claude 3.7 from 76.4% to 92.9% on Aider (+16.5pp), a larger gain than switching to a more expensive model</p>"},{"location":"briefs/ai-coding/coding-pareto-frontiers/#scaling-law-awareness","title":"Scaling Law Awareness","text":"<p>Code generation scales at N^0.35 with diminishing returns at 34B+ parameters (ICLR 2025). Primary bottleneck is data availability, not model size. Mid-tier models deliver ~90% of frontier quality \u2014 the last 10% costs 3-10x more.</p>"},{"location":"briefs/ai-coding/coding-pareto-frontiers/#4-emerging-quality-adjusted-metrics","title":"4. Emerging Quality-Adjusted Metrics","text":"<p>No single \"correct-lines-per-hour\" metric has achieved standard adoption. Emerging frameworks:</p> <ul> <li>CTS-SW (Cost to Serve Software): Total delivery cost including rework, incidents, review overhead</li> <li>DORA rework rate: Added as 5th DORA metric in 2025 specifically for AI-code instability</li> <li>Faros 5-dimension framework: Velocity + Quality + Security + Flow + Satisfaction</li> <li>CodeRabbit 2026 targets: Churn &lt;10%, coverage &gt;80%, complexity &lt;15, defect density &lt;1%</li> </ul> <p>Industry framing shift: CodeRabbit titled their 2026 outlook \"2025 was the year of AI speed. 2026 will be the year of AI quality.\"</p>"},{"location":"briefs/ai-coding/coding-pareto-frontiers/#advisory-assessment","title":"Advisory Assessment","text":""},{"location":"briefs/ai-coding/coding-pareto-frontiers/#for-model-selection","title":"For Model Selection","text":"<ol> <li>No single best model exists. Task-specific evaluation is mandatory \u2014 Claude Opus leads real-world SWE, Gemini leads competitive coding, GPT-5.3 Codex leads terminal tasks.</li> <li>Cascade routing is the optimal strategy for the model frontier (ICLR 2025). The 80/20 rule: 80% of tasks can use cheap/fast models.</li> <li>Gemini 3.1 Pro is the current price-performance champion \u2014 7.5x cheaper than Opus with competitive scores.</li> </ol>"},{"location":"briefs/ai-coding/coding-pareto-frontiers/#for-workflow-design","title":"For Workflow Design","text":"<ol> <li>The workflow frontier matters more than the model frontier. Verification-first workflows are the only demonstrated technique for net-positive quality-adjusted throughput.</li> <li>The \"quality-first AI-assisted\" quadrant is unstudied. This is the largest research gap in the field and the highest-value area for organizational investment.</li> <li>Quantization degrades coding disproportionately \u2014 5-10% accuracy loss, especially on reasoning-heavy tasks and low-resource languages.</li> </ol>"},{"location":"briefs/ai-coding/coding-pareto-frontiers/#key-uncertainties","title":"Key Uncertainties","text":"<ul> <li>Whether the METR -19% result replicates with experienced tool users on newer models</li> <li>Whether the 1.7x defect rate improves with Feb 2026 models (no data yet)</li> <li>The magnitude of quality improvement from multi-agent review architectures</li> <li>Whether CTS-SW or similar composite metrics gain standard adoption</li> </ul> <p>Confidence: High (evidence synthesis), Medium (predictive claims) Would change if: Quality-optimized AI workflow studies are published; newer models demonstrably reduce defect rates; METR RCT replicated with experienced users</p>"},{"location":"briefs/ai-coding/coding-pareto-frontiers/#sources","title":"Sources","text":"Source Type Tier METR 2025 \u2014 AI Developer Productivity RCT Pre-registered RCT 1 METR Feb 2026 \u2014 Transcript Analysis Exploratory analysis 2 Dekoninck et al. ICLR 2025 \u2014 Cascade Routing Peer-reviewed 1 Stanford HAI AI Index 2025 Institutional report 1 Google DORA 2025 Report Institutional report 1 Sonar 2026 Developer Survey (n=1,100) Industry survey 2 Harness AI Velocity Paradox (n=900) Industry survey 2 Uplevel Copilot Study (n=800) Industry research 2 CodeRabbit State of Code Quality (n=470 PRs) Industry report 3 GitClear 2025 (211M LoC) Industry research 2 Greptile State of AI Coding 2025 Industry report 3 SWE-bench Verified / LiveCodeBench / Aider Benchmark leaderboards 3 Artificial Analysis Feb 2026 Benchmark analysis 3 BAMBO arXiv:2512.09972 Preprint 2 Zvi Mowshowitz METR Critique Expert analysis 3 <p>Evidence quality: 25% Tier 1, 45% Tier 2, 30% Tier 3</p>"}]}